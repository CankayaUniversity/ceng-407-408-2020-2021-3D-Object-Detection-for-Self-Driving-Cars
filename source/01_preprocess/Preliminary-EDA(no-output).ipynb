{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Note:\n",
    "Notebook contains huge size of rendered animations and 2D/3D visual files. Due to large size of outputs, we just uploaded source codes of the file. Please refer Github wiki for outputted(full) file. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing necessity packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install shapely -U\n",
    "#!pip install lyft-dataset-sdk\n",
    "#!pip install imagemagick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "from pprint import pprint\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Tuple, List\n",
    "from IPython.display import HTML, Image\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "\n",
    "from matplotlib.axes import Axes\n",
    "from matplotlib import animation, rc\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "from plotly.offline import plot, init_notebook_mode\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "import seaborn as sns\n",
    "from pyquaternion import Quaternion\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Load the  Lyft SDK packages\n",
    "from lyft_dataset_sdk.utils.map_mask import MapMask\n",
    "from lyft_dataset_sdk.lyftdataset import LyftDataset\n",
    "from lyft_dataset_sdk.utils.data_classes import LidarPointCloud\n",
    "from lyft_dataset_sdk.utils.geometry_utils import view_points, box_in_image, BoxVisibility\n",
    "from lyft_dataset_sdk.utils.geometry_utils import view_points, transform_matrix\n",
    "from pathlib import Path\n",
    "\n",
    "import struct\n",
    "from abc import ABC, abstractmethod\n",
    "from functools import reduce\n",
    "from typing import Tuple, List, Dict\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's define working directory \n",
    "BASE_PATH = '/home/keceli/3d-data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the directories under the BASE_PATH\n",
    "dirs = os.listdir(BASE_PATH)\n",
    "print(dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adjust root folders as an alias paramater.\n",
    "!ln -s /kaggle/input/3d-object-detection-for-autonomous-vehicles/test_images images\n",
    "!ln -s /kaggle/input/3d-object-detection-for-autonomous-vehicles/test_maps maps\n",
    "!ln -s /kaggle/input/3d-object-detection-for-autonomous-vehicles/test_lidar lidar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining LyftDataset Class\n",
    "class LyftTestDataset(LyftDataset):\n",
    "    \"\"\"Database class for Lyft Dataset to help query and retrieve information from the database.\"\"\"\n",
    "\n",
    "    def __init__(self, data_path: str, json_path: str, verbose: bool = True, map_resolution: float = 0.1):\n",
    "        \"\"\"Loads database and creates reverse indexes and shortcuts.\n",
    "        Args:\n",
    "            data_path: Path to the tables and data.\n",
    "            json_path: Path to the folder with json files\n",
    "            verbose: Whether to print status messages during load.\n",
    "            map_resolution: Resolution of maps (meters).\n",
    "        \"\"\"\n",
    "\n",
    "        self.data_path = Path(data_path).expanduser().absolute()\n",
    "        self.json_path = Path(json_path)\n",
    "\n",
    "        self.table_names = [\n",
    "            \"category\",\n",
    "            \"attribute\",\n",
    "            \"sensor\",\n",
    "            \"calibrated_sensor\",\n",
    "            \"ego_pose\",\n",
    "            \"log\",\n",
    "            \"scene\",\n",
    "            \"sample\",\n",
    "            \"sample_data\",\n",
    "            \"map\",\n",
    "        ]\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Explicitly assign tables to help the IDE determine valid class members.\n",
    "        self.category = self.__load_table__(\"category\")\n",
    "        self.attribute = self.__load_table__(\"attribute\")\n",
    "        \n",
    "        \n",
    "        self.sensor = self.__load_table__(\"sensor\")\n",
    "        self.calibrated_sensor = self.__load_table__(\"calibrated_sensor\")\n",
    "        self.ego_pose = self.__load_table__(\"ego_pose\")\n",
    "        self.log = self.__load_table__(\"log\")\n",
    "        self.scene = self.__load_table__(\"scene\")\n",
    "        self.sample = self.__load_table__(\"sample\")\n",
    "        self.sample_data = self.__load_table__(\"sample_data\")\n",
    "        \n",
    "        self.map = self.__load_table__(\"map\")\n",
    "\n",
    "        if verbose:\n",
    "            for table in self.table_names:\n",
    "                print(\"{} {},\".format(len(getattr(self, table)), table))\n",
    "            print(\"Done loading in {:.1f} seconds.\\n======\".format(time.time() - start_time))\n",
    "\n",
    "        # Initialize LyftDatasetExplorer class\n",
    "        self.explorer = LyftDatasetExplorer(self)\n",
    "        # Make reverse indexes for common lookups.\n",
    "        self.__make_reverse_index__(verbose)\n",
    "        \n",
    "    def __make_reverse_index__(self, verbose: bool) -> None:\n",
    "        \"\"\"De-normalizes database to create reverse indices for common cases.\n",
    "        Args:\n",
    "            verbose: Whether to print outputs.\n",
    "        \"\"\"\n",
    "\n",
    "        start_time = time.time()\n",
    "        if verbose:\n",
    "            print(\"Reverse indexing ...\")\n",
    "\n",
    "        # Store the mapping from token to table index for each table.\n",
    "        self._token2ind = dict()\n",
    "        for table in self.table_names:\n",
    "            self._token2ind[table] = dict()\n",
    "\n",
    "            for ind, member in enumerate(getattr(self, table)):\n",
    "                self._token2ind[table][member[\"token\"]] = ind\n",
    "\n",
    "        # Decorate (adds short-cut) sample_data with sensor information.\n",
    "        for record in self.sample_data:\n",
    "            cs_record = self.get(\"calibrated_sensor\", record[\"calibrated_sensor_token\"])\n",
    "            sensor_record = self.get(\"sensor\", cs_record[\"sensor_token\"])\n",
    "            record[\"sensor_modality\"] = sensor_record[\"modality\"]\n",
    "            record[\"channel\"] = sensor_record[\"channel\"]\n",
    "\n",
    "        # Reverse-index samples with sample_data and annotations.\n",
    "        for record in self.sample:\n",
    "            record[\"data\"] = {}\n",
    "            record[\"anns\"] = []\n",
    "\n",
    "        for record in self.sample_data:\n",
    "            if record[\"is_key_frame\"]:\n",
    "                sample_record = self.get(\"sample\", record[\"sample_token\"])\n",
    "                sample_record[\"data\"][record[\"channel\"]] = record[\"token\"]\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Done reverse indexing in {:.1f} seconds.\\n======\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Observe dataset features.\n",
    "classes = [\"car\", \"motorcycle\", \"bus\", \"bicycle\", \"truck\", \"pedestrian\", \"other_vehicle\", \"animal\", \"emergency_vehicle\"]\n",
    "train_dataset = LyftDataset(data_path='.', json_path='/home/keceli/3d-data/train_data', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First five row of the train.csv file.\n",
    "train = pd.read_csv('/home/keceli/3d-data/train.csv')\n",
    "sample_submission = pd.read_csv('/home/keceli/3d-data/sample_submission.csv')\n",
    "print(train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**train.csv** file contains **sample_tokens** in the training set with annotations for all objects, we have the following components:\n",
    "\n",
    "* **center_x**, **center_y** and **center_z** represent the 3D the world coordinates .\n",
    "* **width**, **length** and **height** are the dimensions of the observed objects.\n",
    "* **yaw** is the angle of the volume around the z axis\n",
    "* **class_name** is the type of object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_columns = ['sample_id', 'object_id', 'center_x', 'center_y', 'center_z',\n",
    "                  'width', 'length', 'height', 'yaw', 'class_name']\n",
    "objects = []\n",
    "for sample_id, ps in tqdm(train.values[:]):\n",
    "    object_params = ps.split()\n",
    "    n_objects = len(object_params)\n",
    "    for i in range(n_objects // 8):\n",
    "        x, y, z, w, l, h, yaw, c = tuple(object_params[i * 8: (i + 1) * 8])\n",
    "        objects.append([sample_id, i, x, y, z, w, l, h, yaw, c])\n",
    "train_objects = pd.DataFrame(\n",
    "    objects,\n",
    "    columns = object_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = ['object_id', 'center_x', 'center_y', 'center_z', 'width', 'length', 'height', 'yaw']\n",
    "train_objects[numerical_cols] = np.float32(train_objects[numerical_cols].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_objects.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking to train_images first 20 files\n",
    "os.listdir(BASE_PATH + \"/train_images\")[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploring the train data\n",
    "os.listdir(BASE_PATH + \"/train_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sample_data.json** contains all the necessity information about the training data. Let's take a look to the informations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(BASE_PATH + '/train_data/sample_data.json') as f:\n",
    "    data_json = json.load(f)\n",
    "\n",
    "print(len(data_json), \"records found in sample_data.json\")\n",
    "\n",
    "print(\"\\nBelow showing recorded informations about lidar data:\")\n",
    "pprint(data_json[0])\n",
    "\n",
    "print('\\nBelow showing recored informations about image data:')\n",
    "pprint(data_json[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**scene.json** contains necessity information about the scenes. Let's take a look to the informations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(BASE_PATH + '/train_data/scene.json') as f:\n",
    "    scene_json = json.load(f)\n",
    "\n",
    "print(\"There are\", len(scene_json), \"records in sample_data.json\")\n",
    "\n",
    "pprint(scene_json[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in object_columns[2:-1]:\n",
    "    train_objects[col] = train_objects[col].astype('float')\n",
    "train_objects['confidence'] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_objects.shape\n",
    "train_objects.head()\n",
    "train_objects.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3,figsize=(10, 10))\n",
    "sns.distplot(train_objects.center_x, ax = ax[0])\n",
    "sns.distplot(train_objects.center_y, ax = ax[1])\n",
    "sns.distplot(train_objects.center_z, ax = ax[2])\n",
    "plt.suptitle('X, y, z coord distribution')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the dataset, most of the objects are cars. So that, width, length and height distributions skewed to certain range. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3,figsize=(10, 10))\n",
    "sns.distplot(train_objects.width, ax = ax[0])\n",
    "sns.distplot(train_objects.length, ax = ax[1])\n",
    "sns.distplot(train_objects.height, ax = ax[2])\n",
    "plt.suptitle('Width, length, height distribution')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we said above, cars obejects dominating the other class with the annotated targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_cnt = train_objects.groupby('class_name').count()[['object_id']].sort_values(by='object_id', ascending=False).reset_index()\n",
    "class_cnt['p'] = class_cnt.object_id / class_cnt.object_id.sum() \n",
    "class_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_objects.groupby('class_name').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submission string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, z, w, l, h, yaw = train_objects[[\n",
    "    'center_x', 'center_y', 'center_z', 'width', 'length', 'height', 'yaw']].mean()\n",
    "mean_prediction_string = ' '.join(map(str, [0.9, x, y, z, 10*w, 10*l, h, yaw, 'car']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission['PredictionString'] = mean_prediction_string \n",
    "sample_submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.shape\n",
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that for below graph, car's cameras are sensing the objects left or right that because small width of the road."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.distplot(train_objects['center_x'], color='green', ax=ax).set_title('center_ & and center_y', fontsize=16)\n",
    "sns.distplot(train_objects['center_y'], color='red', ax=ax).set_title('center_x & center_y', fontsize=16)\n",
    "plt.xlabel('center_x & center_y', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason could be the camera detect objects either too far ahead or side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_objects = train_objects.query('class_name == \"car\"')\n",
    "plot = sns.jointplot(x=new_train_objects['center_x'][:1000], y=new_train_objects['center_y'][:1000], kind='kde', color='blue')\n",
    "plot.set_axis_labels('center_x', 'center_y', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "center_z is right skewed and  clustured near -20, and also coordinates are negative because, camera sensor placed at the top of the vehicle. So, most of the times, camera sensor look down to see objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.distplot(train_objects['center_z'], color='magenta', ax=ax).set_title('center_z', fontsize=16)\n",
    "plt.xlabel('center_z', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yaw value is the volume of z axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.distplot(train_objects['yaw'], color='darkgreen', ax=ax).set_title('yaw', fontsize=16)\n",
    "plt.xlabel('yaw', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.distplot(train_objects['width'], color='darkblue', ax=ax).set_title('width', fontsize=16)\n",
    "plt.xlabel('width', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.distplot(train_objects['length'], color='yellow', ax=ax).set_title('length', fontsize=16)\n",
    "plt.xlabel('length', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.distplot(train_objects['height'], color='indigo', ax=ax).set_title('height', fontsize=16)\n",
    "plt.xlabel('height', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_objects['class_name'].value_counts().plot(kind='bar');\n",
    "plt.title('Class counts');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_objects['class_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "plot = sns.countplot(y=\"class_name\", data=train_objects.query('class_name != \"motorcycle\" and class_name != \"emergency_vehicle\" and class_name != \"animal\"'),\n",
    "                     palette=['navy', 'darkblue', 'blue', 'dodgerblue', 'skyblue', 'lightblue']).set_title('Object Frequencies', fontsize=16)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xlabel(\"Count\", fontsize=15)\n",
    "plt.ylabel(\"Class Name\", fontsize=15)\n",
    "plt.show(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "\n",
    "plot = sns.boxplot(x=\"class_name\", y=\"center_x\",\n",
    "                   data=train_objects.query('class_name != \"motorcycle\" and class_name != \"emergency_vehicle\" and class_name != \"animal\"'),\n",
    "                   palette='YlGnBu', ax=ax).set_title('center_x (for different objects)', fontsize=16)\n",
    "\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.xlabel(\"Class Name\", fontsize=15)\n",
    "plt.ylabel(\"center_x\", fontsize=15)\n",
    "plt.show(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "\n",
    "plot = sns.boxplot(x=\"class_name\", y=\"center_y\",\n",
    "                   data=train_objects.query('class_name != \"motorcycle\" and class_name != \"emergency_vehicle\" and class_name != \"animal\"'),\n",
    "                   palette='YlOrRd', ax=ax).set_title('center_y (for different objects)', fontsize=16)\n",
    "\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.xlabel(\"Class Name\", fontsize=15)\n",
    "plt.ylabel(\"center_y\", fontsize=15)\n",
    "plt.show(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "\n",
    "plot = sns.boxplot(x=\"class_name\", y=\"center_z\",\n",
    "                   data=train_objects.query('class_name != \"motorcycle\" and class_name != \"emergency_vehicle\" and class_name != \"animal\"').query('center_z <= -5'),\n",
    "                   palette='RdPu', ax=ax).set_title('center_z (for different objects)', fontsize=16)\n",
    "\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.xlabel(\"Class Name\", fontsize=15)\n",
    "plt.ylabel(\"center_z\", fontsize=15)\n",
    "plt.show(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "\n",
    "plot = sns.boxplot(x=\"class_name\", y=\"width\",\n",
    "                   data=train_objects.query('class_name != \"motorcycle\" and class_name != \"emergency_vehicle\" and class_name != \"animal\"'),\n",
    "                   palette='YlGn', ax=ax).set_title('width (for different objects)', fontsize=16)\n",
    "\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.xlabel(\"Class Name\", fontsize=15)\n",
    "plt.ylabel(\"width\", fontsize=15)\n",
    "plt.show(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "\n",
    "plot = sns.boxplot(x=\"class_name\", y=\"length\",\n",
    "                   data=train_objects.query('class_name != \"motorcycle\" and class_name != \"emergency_vehicle\" and class_name != \"animal\" and length < 15'),\n",
    "                   palette='Purples', ax=ax).set_title('length (for different objects)', fontsize=16)\n",
    "\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.xlabel(\"Class Name\", fontsize=15)\n",
    "plt.ylabel(\"length\", fontsize=15)\n",
    "plt.show(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "\n",
    "plot = sns.boxplot(x=\"class_name\", y=\"height\",\n",
    "                   data=train_objects.query('class_name != \"motorcycle\" and class_name != \"emergency_vehicle\" and class_name != \"animal\" and height < 6'),\n",
    "                   palette='Reds', ax=ax).set_title('height (for different objects)', fontsize=16)\n",
    "\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.xlabel(\"Class Name\", fontsize=15)\n",
    "plt.ylabel(\"height\", fontsize=15)\n",
    "plt.show(plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Animating 2D/3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metadata of the scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.list_scenes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palo_alto = train_dataset.scene[0]\n",
    "first_sample_token = palo_alto[\"first_sample_token\"]\n",
    "train_dataset.render_sample(first_sample_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sample = train_dataset.get('sample', first_sample_token)\n",
    "my_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.render_pointcloud_in_image(sample_token=first_sample_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will project LiDAR and Camera data into one image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor = 'CAM_FRONT'\n",
    "cam_front = train_dataset.get('sample_data', my_sample['data'][sensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.render_sample_data(cam_front['token'], with_anns=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.render_pointcloud_in_image(sample_token = my_sample[\"token\"],\n",
    "                                      dot_size = 1,\n",
    "                                      camera_channel = 'CAM_BACK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a particular sensor of sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_channel = 'CAM_FRONT'\n",
    "my_sample_data = train_dataset.get('sample_data', my_sample['data'][sensor_channel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.render_sample_data(my_sample_data['token'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.render_sample_data(my_sample_data['token'], with_anns=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's render different annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_ann_token = my_sample['anns'][25]\n",
    "my_ann =  my_sample_data.get('sample_annotation', my_ann_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.render_annotation(my_ann)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will visualize lidar data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes_limits = [\n",
    "    [-30, 50], # X axis range\n",
    "    [-30, 20], # Y axis range\n",
    "    [-3, 10]   # Z axis range\n",
    "]\n",
    "axes_str = ['X', 'Y', 'Z']\n",
    "\n",
    "def display_frame_statistics(lidar_points, points=0.2):\n",
    "    \"\"\"\n",
    "    Displays statistics for a single frame. Draws 3D plot of the lidar point cloud data and point cloud\n",
    "    projections to various planes.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lidar_points: lidar data points \n",
    "    points          : Fraction of lidar points to use. Defaults to `0.2`, e.g. 20%.\n",
    "    \"\"\"\n",
    "    \n",
    "    points_step = int(1. / points)\n",
    "    point_size = 0.01 * (1. / points)\n",
    "    pc_range = range(0, lidar_points.shape[1], points_step)\n",
    "    pc_frame = lidar_points[:, pc_range]\n",
    "    def draw_point_cloud(ax, title, axes=[0, 1, 2]):\n",
    "        \"\"\"Convenient method for drawing various point cloud projections as a part of frame statistics\"\"\"\n",
    "        ax.set_facecolor('black')\n",
    "        ax.grid(False)\n",
    "        ax.scatter(*pc_frame[axes, :], s=point_size, c='white', cmap='grey')\n",
    "        if len(axes) == 3: # 3D configs\n",
    "            text_color = 'white'\n",
    "            ax.set_xlim3d([-10, 30])\n",
    "            ax.set_ylim3d(*axes_limits[axes[1]])\n",
    "            ax.set_zlim3d(*axes_limits[axes[2]])\n",
    "            ax.set_zlabel('{} axis'.format(axes_str[axes[2]]), color='white')\n",
    "        else: # 2D configs\n",
    "            text_color = 'black' # the `figure` is white\n",
    "            ax.set_xlim(*axes_limits[axes[0]])\n",
    "            ax.set_ylim(*axes_limits[axes[1]])\n",
    "        ax.set_title(title, color=text_color)\n",
    "        ax.set_xlabel('{} axis'.format(axes_str[axes[0]]), color=text_color)\n",
    "        ax.set_ylabel('{} axis'.format(axes_str[axes[1]]), color=text_color)\n",
    "            \n",
    "    # Draw point cloud data as 3D plot\n",
    "    f2 = plt.figure(figsize=(15, 8))\n",
    "    ax2 = f2.add_subplot(111, projection='3d')\n",
    "    # make the panes transparent\n",
    "    ax2.xaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n",
    "    ax2.yaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n",
    "    ax2.zaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n",
    "    draw_point_cloud(ax2, '3D plot')\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close(f2)\n",
    "    # Draw point cloud data as plane projections\n",
    "    f, ax3 = plt.subplots(3, 1, figsize=(15, 25))\n",
    "#     f.set_facecolor('black')\n",
    "    draw_point_cloud(\n",
    "        ax3[0], \n",
    "        'XZ projection (Y = 0)', #, the car is moving in direction left to right', ?\n",
    "        axes=[0, 2] # X and Z axes\n",
    "    )\n",
    "    draw_point_cloud(\n",
    "        ax3[1], \n",
    "        'XY projection (Z = 0)', #, the car is moving in direction left to right',? \n",
    "        axes=[0, 1] # X and Y axes\n",
    "    )\n",
    "    draw_point_cloud(\n",
    "        ax3[2], \n",
    "        'YZ projection (X = 0)', #, the car is moving towards the graph plane', ?\n",
    "        axes=[1, 2] # Y and Z axes\n",
    "    )\n",
    "    plt.show()\n",
    "    plt.close(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lidar_top = train_dataset.get('sample_data', my_sample['data']['LIDAR_TOP'])\n",
    "pc = LidarPointCloud.from_file(Path(lidar_top['filename']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_frame_statistics(pc.points, points=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.render_sample_data(lidar_top['token'], with_anns=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.render_sample_data(lidar_top['token'], with_anns=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's store lidar and image data in seperate frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lidar_dataset = []\n",
    "image_dataset = []\n",
    "\n",
    "for data in data_json:\n",
    "    if data['fileformat'] == 'bin':\n",
    "        lidar_dataset.append(data)\n",
    "    else:\n",
    "        image_dataset.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lidar_df = pd.DataFrame(lidar_dataset)\n",
    "image_df = pd.DataFrame(image_dataset)\n",
    "\n",
    "print(lidar_df.shape)\n",
    "print(image_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lidar_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will augment image_df which will provide information about camera and host\n",
    "image_df['host'] = image_df['filename'].apply(lambda st: st.strip('images/host-').split('_')[0])\n",
    "image_df['cam'] = image_df['filename'].apply(lambda st: st.split('_')[1])\n",
    "image_df['timestamp'] = image_df['filename'].apply(lambda st: st.split('_')[2].strip('.jpeg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will store frames which we had created.\n",
    "image_df.to_csv(\"/home/keceli/3d-data/sample_data_images.csv\")\n",
    "lidar_df.to_csv(\"/home/keceli/3d-data/lidar_data_images.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_df['host'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_df['cam'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_host_sample(host, n_images, jumps=1):\n",
    "    cams = list(sorted(image_df['cam'].unique()))\n",
    "    \n",
    "    fig, axs = plt.subplots(\n",
    "        n_images, len(cams), figsize=(3*len(cams), 3*n_images), \n",
    "        sharex=True, sharey=True, gridspec_kw = {'wspace':0.1, 'hspace':0.1}\n",
    "    )\n",
    "    \n",
    "    for i in range(n_images):\n",
    "        for c, cam in enumerate(cams):\n",
    "            if i == 0:\n",
    "                axs[i, c].set_title(cam)\n",
    "            \n",
    "            mask1 = image_df.cam == cam\n",
    "            mask2 = image_df.host == host\n",
    "            image_path = image_df[mask1 & mask2]\n",
    "            image_path = image_path.sort_values('timestamp')['filename'].iloc[i*jumps]\n",
    "            \n",
    "            img = cv2.imread('/home/keceli/3d-data/train_' + image_path)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = cv2.resize(img, (200, 200))\n",
    "            \n",
    "            axs[i, c].imshow(img)\n",
    "            axs[i, c].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_host_sample('005', 5, jumps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_host_sample('101', 5, jumps=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_host_sample('009', 5, jumps=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_token(scene):\n",
    "    scene = train_dataset.scene[scene]\n",
    "    sample_token = scene['first_sample_token']\n",
    "    sample_record = train_dataset.get(\"sample\", sample_token)\n",
    "    \n",
    "    while sample_record['next']:\n",
    "        sample_token = sample_record['next']\n",
    "        sample_record = train_dataset.get(\"sample\", sample_token)\n",
    "        \n",
    "        yield sample_token\n",
    "\n",
    "def animate_images(scene, frames, pointsensor_channel='LIDAR_TOP', interval=1):\n",
    "    cams = [\n",
    "        'CAM_FRONT',\n",
    "        'CAM_FRONT_RIGHT',\n",
    "        'CAM_BACK_RIGHT',\n",
    "        'CAM_BACK',\n",
    "        'CAM_BACK_LEFT',\n",
    "        'CAM_FRONT_LEFT',\n",
    "    ]\n",
    "\n",
    "    generator = generate_next_token(scene)\n",
    "\n",
    "    fig, axs = plt.subplots(\n",
    "        2, len(cams), figsize=(3*len(cams), 6), \n",
    "        sharex=True, sharey=True, gridspec_kw = {'wspace': 0, 'hspace': 0.1}\n",
    "    )\n",
    "    \n",
    "    plt.close(fig)\n",
    "\n",
    "    def animate_fn(i):\n",
    "        for _ in range(interval):\n",
    "            sample_token = next(generator)\n",
    "            \n",
    "        for c, camera_channel in enumerate(cams):    \n",
    "            sample_record = train_dataset.get(\"sample\", sample_token)\n",
    "\n",
    "            pointsensor_token = sample_record[\"data\"][pointsensor_channel]\n",
    "            camera_token = sample_record[\"data\"][camera_channel]\n",
    "            \n",
    "            axs[0, c].clear()\n",
    "            axs[1, c].clear()\n",
    "            \n",
    "            train_dataset.render_sample_data(camera_token, with_anns=False, ax=axs[0, c])\n",
    "            train_dataset.render_sample_data(camera_token, with_anns=True, ax=axs[1, c])\n",
    "            \n",
    "            axs[0, c].set_title(\"\")\n",
    "            axs[1, c].set_title(\"\")\n",
    "\n",
    "    anim = animation.FuncAnimation(fig, animate_fn, frames=frames, interval=interval)\n",
    "    \n",
    "    return anim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "anim = animate_images(scene=0, frames=100, interval=1)\n",
    "HTML(anim.to_jshtml(fps=8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim.save('/home/keceli/3d-data/7.gif', fps = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim = animate_images(scene=10, frames=100, interval=1)\n",
    "HTML(anim.to_jshtml(fps=8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim = animate_images(scene=50, frames=100, interval=1)\n",
    "HTML(anim.to_jshtml(fps=8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim = animate_images(scene=100, frames=100, interval=1)\n",
    "HTML(anim.to_jshtml(fps=8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Animate Lidar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def animate_lidar(scene, frames, pointsensor_channel='LIDAR_TOP', with_anns=True, interval=1):\n",
    "    generator = generate_next_token(scene)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 1, figsize=(8, 8))\n",
    "    plt.close(fig)\n",
    "\n",
    "    def animate_fn(i):\n",
    "        for _ in range(interval):\n",
    "            sample_token = next(generator)\n",
    "        \n",
    "        axs.clear()\n",
    "        sample_record = train_dataset.get(\"sample\", sample_token)\n",
    "        pointsensor_token = sample_record[\"data\"][pointsensor_channel]\n",
    "        train_dataset.render_sample_data(pointsensor_token, with_anns=with_anns, ax=axs)\n",
    "\n",
    "    anim = animation.FuncAnimation(fig, animate_fn, frames=frames, interval=interval)\n",
    "    \n",
    "    return anim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "anim = animate_lidar(scene=0, frames=100, interval=1)\n",
    "HTML(anim.to_jshtml(fps=8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "anim = animate_lidar(scene=10, frames=100, interval=1)\n",
    "HTML(anim.to_jshtml(fps=8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim = animate_lidar(scene=50, frames=100, interval=1)\n",
    "HTML(anim.to_jshtml(fps=8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "anim = animate_lidar(scene=100, frames=100, interval=1)\n",
    "HTML(anim.to_jshtml(fps=8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url='/home/keceli/3d-data/a.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
